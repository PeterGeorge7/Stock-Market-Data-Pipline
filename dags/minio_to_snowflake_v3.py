import os
import glob
from airflow.decorators import dag, task
from airflow.operators.python import PythonOperator
from airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.snowflake.hooks.snowflake import SnowflakeHook
from datetime import datetime, timedelta


# Configuration
BUCKET_NAME = "bronze-transactions"
SOURCE_FOLDER = ""
ARCHIVE_FOLDER = "archive/"
LOCAL_DIR = "/tmp/minio_downloads"
SNOWFLAKE_TABLE = "bronze_stock_quotes_raw"
SNOWFLAKE_STAGE = f"@%{SNOWFLAKE_TABLE}"
SNOWFLAKE_CONN_ID = "snowflake_conn"
MINIO_CONN_ID = "minio_conn"


@dag(
    "minio_to_snowflake_v3",
    default_args={
        "owner": "airflow",
        "start_date": datetime(2025, 1, 1),
        # "retries": 1,
        # "retry_delay": timedelta(minutes=1),
    },
    schedule="*/5 * * * *",
    catchup=False,
)
def minio_to_snowflake_v3():

    # downloads files from MinIO
    @task()
    def download_files():
        s3_hook = S3Hook(aws_conn_id=MINIO_CONN_ID)

        keys = s3_hook.list_keys(bucket_name=BUCKET_NAME, prefix=SOURCE_FOLDER)

        if not keys:
            print("No files found in bucket. Skipping.")
            return {"downloaded_files": [], "processing_keys": []}
        # Filter out archived files, to process only new files
        processing_keys = [
            k for k in keys if not k.startswith(ARCHIVE_FOLDER) and k.endswith(".json")
        ]

        if not processing_keys:
            print("No new files found. Skipping.")
            return {"downloaded_files": [], "processing_keys": []}

        print(f"Preparing to move {len(processing_keys)} files to Snowflake Stage...")

        os.makedirs(LOCAL_DIR, exist_ok=True)

        downloaded_files = []
        for key in processing_keys:
            actual_path = s3_hook.download_file(
                key=key,
                bucket_name=BUCKET_NAME,
                local_path=LOCAL_DIR,
                preserve_file_name=True,
                # we can use use_autogenerated_subdir=False
            )
            downloaded_files.append(actual_path)

        # Return dictionary for XCom (TaskFlow API handles this automatically)
        return {
            "downloaded_files": downloaded_files,
            "processing_keys": processing_keys,
        }

    # uploads files to Snowflake Stage
    @task()
    def upload_to_snowflake_stage(download_result: dict):
        downloaded_files = download_result.get("downloaded_files", [])
        processing_keys = download_result.get("processing_keys", [])

        if not downloaded_files:
            print("No files to upload to Snowflake Stage. Exiting task.")
            return False

        s3_hook = S3Hook(aws_conn_id=MINIO_CONN_ID)
        snow_hook = SnowflakeHook(snowflake_conn_id=SNOWFLAKE_CONN_ID)

        print("Uploading to Snowflake Stage...")
        # Upload each file individually using the actual downloaded paths
        for file_path in downloaded_files:
            print(f"Uploading file: {file_path}")
            upload_sql = f"PUT 'file://{file_path}' {SNOWFLAKE_STAGE} AUTO_COMPRESS=TRUE OVERWRITE=TRUE"
            snow_hook.run(upload_sql)

        print("Archiving files in MinIO")
        for key in processing_keys:
            new_key = f"{ARCHIVE_FOLDER}{os.path.basename(key)}"
            s3_hook.copy_object(
                source_bucket_key=key,
                dest_bucket_key=new_key,
                source_bucket_name=BUCKET_NAME,
                dest_bucket_name=BUCKET_NAME,
            )
            s3_hook.delete_objects(BUCKET_NAME, key)

        for f in downloaded_files:
            if os.path.exists(f):
                os.remove(f)

        return True

    # loads data into Snowflake table
    @task()
    def load_table_data(upload_result: bool):
        if not upload_result:
            print("No data was uploaded. Skipping COPY INTO.")
            return False

        snow_hook = SnowflakeHook(snowflake_conn_id=SNOWFLAKE_CONN_ID)
        sql = f"""
            COPY INTO {SNOWFLAKE_TABLE}
            FROM {SNOWFLAKE_STAGE}
            FILE_FORMAT = (TYPE = JSON)
            PATTERN = '.*\\.json\\.gz'
            ON_ERROR = 'CONTINUE';            
        """
        snow_hook.run(sql)
        print("Data loaded successfully into Snowflake table.")
        return True

    # runs dbt run after loading data
    @task.bash
    def run_dbt_if_loaded(load_result: bool) -> str:

        # Path inside the Astro/Docker container
        dbt_project_path = "/usr/local/airflow/dags/dbt/stock_mds"
        profiles_dir = "/home/astro/.dbt"

        # Clean cache, install deps, then run dbt
        return f"""
            cd {dbt_project_path} && \
            rm -rf target/ dbt_packages/ logs/ && \
            dbt deps --profiles-dir {profiles_dir} && \
            dbt run --profiles-dir {profiles_dir}
        """

    download_result = download_files()
    upload_task = upload_to_snowflake_stage(download_result)
    load_result = load_table_data(upload_task)
    run_dbt_if_loaded(load_result)


minio_to_snowflake_v3()
